{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "고준서_2018310412_0.91",
      "provenance": [],
      "collapsed_sections": [
        "3vKdBn7G3ldg",
        "iy_u9W8MFzmE",
        "kSlwEN06GN48"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoy36bY3ldf",
        "colab_type": "text"
      },
      "source": [
        "# NLP 중간고사 대체과제 아마존 리뷰 분류기 만들기\n",
        "2018310412 인공지능융합전공 고준서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKdBn7G3ldg",
        "colab_type": "text"
      },
      "source": [
        "#### pandas를 활용하여 csv파일 불러온 후, 처리한다.\n",
        "\n",
        "1.   로컬에서 할 경우 같은 경로에 있으면 되는데, 여긴 드라이브 연동으로 실행하여야 함\n",
        "2.   불러온 후 pos, neg로 되어있는 것을 0 1로 처리한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Hunlhm3ldh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13g_iIar3naf",
        "colab_type": "code",
        "outputId": "1be0e816-c039-42db-8ba8-c51071c9b959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"names\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMI0mNDb50It",
        "colab_type": "code",
        "outputId": "ff7110aa-b49e-4561-ed72-05586e0ddf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo64LnD83ldm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/gdrive/My Drive/Colab Notebooks/train.csv', 'r') as rt:\n",
        "    df_train = pd.read_csv(rt)\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/test.csv', 'r') as et:\n",
        "    df_test = pd.read_csv(et)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GB8eBgc3ldp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = list(df_train[\"text\"])\n",
        "train_sent = list(df_train[\"senti\"])\n",
        "test_text = list(df_test[\"text\"])\n",
        "test_sent = list(df_test[\"senti\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mihA1-H63ldr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pos와 neg로 되어있기 때문에 0과 1로 바꿔준다.\n",
        "for i in range(len(train_sent)):\n",
        "    if train_sent[i] == \"pos\":\n",
        "        train_sent[i] = 0\n",
        "    else:\n",
        "        train_sent[i] = 1\n",
        "        \n",
        "for i in range(len(test_sent)):\n",
        "    if test_sent[i] == \"pos\":\n",
        "        test_sent[i] = 0\n",
        "    else:\n",
        "        test_sent[i] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3KIqHH_3ldu",
        "colab_type": "code",
        "outputId": "ae1442e7-8ed4-4c58-bae7-ac4862632800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_text[1]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I loved this dress. i kept putting it on trying to figure out how to get my boobs to not fall out all over the place. this is not a dress for larger breasts. it is absolutely beautiful otherwise!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy_u9W8MFzmE",
        "colab_type": "text"
      },
      "source": [
        "### 단어 전처리 - 소문자화, lemmatize, 이름 제거 수행\n",
        "그러나, 이 부분을 하지 않는게 성능이 더 좋아서 이 부분은 만들어놓고 실제로 실행은 하지 않았다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv_BunAy3ldx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.corpus import names\n",
        "all_names = set(names.words())\n",
        "\n",
        "def only_alpha(text):\n",
        "    a = \"\"\n",
        "    for i in text:\n",
        "        if i.isalpha():\n",
        "            a += i\n",
        "    return a\n",
        "\n",
        "def clean_text(text):\n",
        "    #단어단위 분절이 아닌 것이 들어와야 한다.\n",
        "    text = text.split(\" \")\n",
        "    ret_doc = []\n",
        "    for word in text:\n",
        "        word = only_alpha(word.lower())\n",
        "        \n",
        "        if word not in all_names and len(word) > 2:\n",
        "            ret_doc.append(word)\n",
        "    return \" \".join(ret_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NbQHYCvfPcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(train_text)):\n",
        "#     train_text[i] = clean_text(train_text[i])\n",
        "\n",
        "# for i in range(len(test_text)):\n",
        "#     test_text[i] = clean_text(test_text[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSlwEN06GN48",
        "colab_type": "text"
      },
      "source": [
        "### 제일 좋은 Naive Bayes 파라미터가 무엇인지 탐색\n",
        "best parameter를 아래에 삽입하여 모델을 만듬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1iX4EYI3ld6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "nb_ = MultinomialNB()\n",
        "pipeline = Pipeline([\n",
        "    (\"Vec\", CountVectorizer(lowercase=True)),\n",
        "    (\"nb\", MultinomialNB()),\n",
        "])\n",
        "parameters_pipeline = {\n",
        "    \"nb__alpha\":(0.001, 0.01, 0.1, 1, 10, 100),\n",
        "    \"nb__fit_prior\":(True, False),\n",
        "    \"Vec__max_df\":(0.1,0.25,0.5, 1.0, 10, 100),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8x7w3Os3ld8",
        "colab_type": "code",
        "outputId": "17484c3f-b48f-4a10-b8f6-7f01a371a797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "grid_search = GridSearchCV(pipeline, parameters_pipeline, n_jobs=-1, cv = 3)\n",
        "grid_search.fit(train_text, train_sent)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('Vec',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern='(?u)\\...w+\\\\b',\n",
              "                                                        tokenizer=None,\n",
              "                                                        vocabulary=None)),\n",
              "                                       ('nb',\n",
              "                                        MultinomialNB(alpha=1.0,\n",
              "                                                      class_prior=None,\n",
              "                                                      fit_prior=True))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'Vec__max_df': (0.1, 0.25, 0.5, 1.0, 10, 100),\n",
              "                         'nb__alpha': (0.001, 0.01, 0.1, 1, 10, 100),\n",
              "                         'nb__fit_prior': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RitSBM07JFh",
        "colab_type": "code",
        "outputId": "7d62ee4c-a248-4aee-c0b2-983ac0f3050f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Vec__max_df': 1.0, 'nb__alpha': 1, 'nb__fit_prior': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeh7qSAAr9He",
        "colab_type": "code",
        "outputId": "e9351ee5-7dae-4df5-90c8-bfba62775f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9485022338363315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTjTIGdNGaTG",
        "colab_type": "text"
      },
      "source": [
        "### 실제로 구해진 파라미터를 적용\n",
        "\n",
        "\n",
        "1.   'nb__alpha': 1\n",
        "2.   'nb__fit_prior': False\n",
        "3.   'Vec__max_df': 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMc6WxKo3ld0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "CV = CountVectorizer(lowercase=True, max_df=1.0)\n",
        "term_docs_train = CV.fit_transform(train_text)\n",
        "term_docs_test = CV.transform(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KPloycr3ld3",
        "colab_type": "code",
        "outputId": "98f6588b-02de-42e8-ac55-25dcec4f71b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#최적의 결과를 바탕으로 설정해줌\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB(alpha = 1, fit_prior=False)\n",
        "nb.fit(term_docs_train, train_sent)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1, class_prior=None, fit_prior=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDIgjalpHU0V",
        "colab_type": "text"
      },
      "source": [
        "### 만들어진 모델 평가 -> 최대 95.037%의 정확도, 매크로평균 91%를 기록한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhDthEj_lOMI",
        "colab_type": "code",
        "outputId": "051848e7-7a09-43e6-e470-3b60dd841e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def evaluate(test_x, test_y, model):\n",
        "    predictions = model.predict(test_x)\n",
        "    print(classification_report(test_y, predictions))\n",
        "    print(\"Macro F1 score - {0:0.5f}\".format(classification_report(test_y, predictions, output_dict=True)[\"macro avg\"][\"f1-score\"]))\n",
        "evaluate(term_docs_test, test_sent, nb)\n",
        "accuracy = nb.score(term_docs_test, test_sent)\n",
        "\n",
        "print(\"accuracy: %0.3f\"%(accuracy*100),\"%\")"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      1260\n",
            "           1       0.80      0.91      0.85       231\n",
            "\n",
            "    accuracy                           0.95      1491\n",
            "   macro avg       0.89      0.93      0.91      1491\n",
            "weighted avg       0.95      0.95      0.95      1491\n",
            "\n",
            "Macro F1 score - 0.91023\n",
            "accuracy: 95.037 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RtOrx8mj5BQ",
        "colab_type": "text"
      },
      "source": [
        "### 이것은 잘 나온 조합 중 하나입니다.\n",
        "\n",
        "1번조합: CountVectorizer + MultinomialNB = 95.0(정확도) / 91.0 (메크로 F1)\n",
        "\n",
        "2번조합: TfidfVectorizer + SVC  = 95.5 / 90.9 (메크로 F1)\n",
        "\n",
        "-> 그외 딥러닝이나 벡터라이징 방식을 교차해서 해보기도 하였지만 위의 두 조합이 가장 좋은 방식이었다.\n",
        "\n",
        "2번 조합이 정확도 자체는 더 높으나, 우리가 높여야 하는 것은 macro F1-score이다.\n",
        "\n",
        "두 방식 모두 report상에서 91로 동일해보이나, 소수점까지 계산하게 되면 \n",
        "\n",
        "1번조합이 91.01% 2번조합이 90.92%로 1번조합이 약간 더 좋다는 것을 확인할 수 있다.\n",
        "\n",
        "\n",
        "## 결론 - Macro F1 Score = 91.02%"
      ]
    }
  ]
}
